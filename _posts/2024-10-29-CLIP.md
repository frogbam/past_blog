---
title: CLIP (Contrastive Language-Image Pre-training) 리뷰
date: 2024-10-29 17:28:00 +09:00
categories: [논문 리뷰, VLM]
tags:
  [vlm, clip]
toc: true
published: true
---

해당 포스트에서는 OpenAI의 CLIP 논문을 요약하는 글을 써보려 한다.  
1~2년전부터 꾸준히 Vision Language Model에 대한 관심과 연구가 증가하고 있는 상황에서, CLIP은 이러한 연구들에서 자주 언급되는 아주 중요한 연구/논문이다.  
이전에 NLP 관련 모델을 가지고 놀아본 적이 있었는데, 확실히 Vision 분야보다 더 앞서고 있는 부분들이 많다는 생각이 들었다. CLIP은 NLP에서 성공한 방식을 Vision분야에 가지고와 적용시켜보는 내용을 포함하고 있다. 내용을 참고하여 Vision 연구의 방향은 어떻게 변할까 나름대로 생각해보는 시간을 갖는 것도 좋을 것이다.

## Background

CLIP 연구의 동기 및 배경을 정리하자면 다음과 같다.

> 1. NLP에서 방대한 양의 데이터를 학습가능케하여 커다란 성공을 보여준 scalable pre-training 혹은 Task-agnostic objectives 방법이 vision에서도 효과적일까?
> 2. 언어적 정보가 vision 분야의 문제에 효과적으로 활용 될 수 있을까?

위의 내용을 논문에서는 어떻게 말하고 있는지 알아보자.

### Task-agnostic objectives
![alt text](image-2.png)

Task-agnostic objectives는 번역, 텍스트 생성 등 구체적인 task를 직접 학습하는 것이 아니라, 언어 전반의 일반적인 패턴, 문법, 의미를 학습하도록 설계하는것을 말한다. 이를 통해 모델은 특정작업에 아주적은 양의 훈련 또는 훈련 없이 다양한 task에 유연하게 적용될 수 있다. 또한, 방대한 양의 텍스트 데이터를 별도의 labeling 작업 없이 그대로 모델에 학습이 가능하다는 아주 큰 장점을 가지고 있다.

Task-agnostic objectives의 가장 대표적인 예는 BERT의 masked language model이다. 문장에 임의로 masking을 하여 해당 mask에 어떤 단어가 들어갈지 모델이 예측하도록 학습하는 방식이다.

해당 학습 방법은 transformer와 함께 NLP분야에서 엄청난 성공을 보여주었다. 당장 우리가 사용하고 있는 ChatGPT만 봐도 알 수 있는 사실이다.

이러한 결과는 고품질 crowd-labeled NLP dataset을 활용한 지도(supervision)보다 web-scale의 방대한 text 데이터를 활용한 총체적인 지도(aggregate supervision)가 더 뛰어남을 시사한다.

그러나 반대로 computer vision에서는 ImageNet과 같은 잘 정제된 적은양의 데이터(crowd-labeled)를 가지고 모델을 pre-train하는 방법이 표준으로 자리잡고있다.

이러한 배경속에서 해당 방법이 computer vision에서도 잘 적용될 수 있을지, 이러한 학습 방식으로 NLP에서 성공을 거둔 언어지식이 vision 문제에도 효과적일지 의문이 드는건 당연한 일이다.


## Approach
### WebImageText(WIT) dataset

natural language supervision의 큰 motivation 중 하나는 대규모의 데이터였다.

하지만 안타깝게도 Vision 분야에서 주로 사용해 왔던 crowd-labeled dataset들은 데이터 양이 작고 학습에 사용할 text 정보가 충분치 않았다.
이를 해결하기 위해 위키피디아에서 약 4억개의 (이미지, 텍스트) 쌍을 수집하여 WebImageText(WIT) 데이터셋을 만들어 활용하였다.

### Selecting an efficient pre-training method
각 task에서 SOTA급 성능을 내는 computer vision 시스템들은 보통 계산량이 매우 크다. 일례로 1000개의 ImageNet class를 예측하는 SOTA모델을 학습하는데 여러 GPU를 동원해 년 단위로 학습을 해야했다.

ImageNet에서도 이정도인데 사실상 무한에 가까운 natural language에서의 visual concept를 학습시키기에는 사실상 불가능하다. 따라서 natural language supervision의 확장성 및 vision task에서의 효과를 확인하기 위해서는 모델의 학습 효율성이 매우 중요할것이라고 말하였다.


![alt text](efficient_graph.png)

위의 이미지는 저자들이 실험한 모델들의 학습 효율성이다.

#### Bag of Words Prediction
이미지에 대한 text를 비교적 간단한 bag-of-words 형식으로 예측하도록 하는 방식. baseline으로 활용되었음.

#### Transformer Language Model
VirTex와 같이 image CNN과 text transformer로 모델을 구성해서 image에 대한 정답 word를 정확히 예측하도록 학습하는 방식. baseline보다 3배 더 비효율적임을 보여주었음.

#### Bag of Words Constrative (CLIP)
최근 연구에서 constrative learning 방법은 정확한 답을 찾도록 학습하는 방법보다 representation을 학습하는데 더 효과적임을 보여주었음.
또한, 효율적인 측면에서도 더 좋은 결과를 보여주었음.

![alt text](clip_constrative.png)

이를 반영해서 정확한 단어를 예측하는 대신, 하나의 배치에서 N개의 image-text pair가 있을 때, N*N간의 similarity를 예측하도록 함. 

이를 위해 N개의 정답 조합에 대해서는 cosine similarity를 maximize 하도록하고, 반대의 조합 N^2-N 개의 조합에 대해서는 minimize 하도록 이미지 인코더와 텍스트 인코더를 학습시킴. 이를 통해 CLIP이 Multi-modal embedding space를 학습할 수 있도록 함


## Experiments
CLIP 논문의 반절이상이 실험내용으로 구성되어 있을 정도로 실험 부분의 분량이 어마어마하다. OpenAI의 강력한 하드웨어 덕분인지 여러 모델들과의 여러 세팅에서의 실험들을 확인할 수 있다. 그 중 일부 실험들만 가져와 작성해본다.

### Zero-Shot CLIP vs Linear Probe on ResNet50
![alt text](image-3.png)
27개의 데이터셋에서 CLIP 모델의 Zero-Shot 예측 결과와, ResNet-50의 특징 추출부분은 ImageNet으로 학습시키고 Linear Classifier를 각각의 dataset에서 학습한 모델의 예측 결과를 비교하였다.

27개의 dataset중 16개에서 Zero-Shot CLIP이 더 나은 성능을 보여주었다.

위성 이미지 분류(EuroSAT, RESISC45), 림프절 종양 검출(PatchCamelyon), 합성 장면에서 객체 수 세기(CLEVRCounts), 독일 교통 표지판 인식(GTSRB), 가까운 차량 거리 인식(KITTI Distance) 같은 특수하고 복잡하거나 추상적인 작업에서 성능이 낮았다. 이는 복잡한 작업에 대한 zero-shot CLIP의 한계를 보여주었다.


### Zero-Shot vs Few-Shot
![alt text](image-4.png)

Zero-Shot CLIP은 4-Shot CLIP 모델과 성능이 비슷하게 나왔다. 이는 Zero-Shot 접근이 자연어를 통해 시각적 개념을 직접적으로 전달하기 때문에 가능했다. 

CLIP은 자연어를 사용하여 개념을 명시적으로 전달하는 반면, 일반적인 감독 학습은 훈련 예시를 통해 개념을 간접적으로 추론해야 한다.

Zero-Shot CLIP은 BiT-M의 16-Shot 성능과 비슷했다.


### Representation 비교
![alt text](image.png)
학습된 CLIP 모델에 선형 분류기를 붙여 각 데이터셋에서 선형 분류기만 학습하고, 표현(Representation) 부분은 고정하였다.

세밀한 객체 분류, OCR, 동영상에서의 활동 인식, 위치 기반 분류 등 27개의 데이터셋을 대상으로 한 테스트에서, CLIP 모델이 더 폭넓게 유용한 이미지 표현을 학습한다는 것을 확인했다. 또한, CLIP 모델은 비교 모델들 보다 계산 효율성도 더 높았다.
### Robustness to Natural Distribution Shift
![alt text](image-1.png)

## Limitations
1. Zero-Shot 성능 한계: Zero-shot CLIP은 특정 작업에서 ImageNet으로 학습된 ResNet-50 기반의 선형 분류기와 유사한 성능을 보이지만, 여전히 최첨단 성능에는 미치지 못한다. 특히, 세밀한 객체 분류나 추상적 작업(예: 객체 수 세기)에서는 성능이 낮다. 새롭거나 훈련 데이터에 포함되지 않은 작업에서는 성능이 거의 무작위 수준까지 떨어진다.

2. 일반화의 취약성: 자연 이미지에는 잘 일반화되지만, 손글씨 인식 같은 진정한 분포 외 데이터에 대해서는 성능이 낮다. 예를 들어, MNIST 데이터셋의 경우 CLIP은 디지털 텍스트에는 잘 작동하지만, 손글씨 인식 정확도는 88%에 그쳐 단순한 로지스틱 회귀보다도 낮은 성능을 보인다.

3. 유연성 한계: CLIP은 다양한 작업에 대해 zero-shot 분류기를 생성할 수 있지만, 이미 주어진 개념 안에서만 선택할 수 있어 유연성이 제한된다. 이미지 캡션 모델처럼 새로운 출력을 생성하지는 못하며, 효율성과 유연성을 동시에 높이기 위한 개선이 필요하다.

4. 데이터 효율성 문제: CLIP은 데이터 효율성이 낮아 대규모 데이터로 보완했지만, 이는 막대한 계산 자원을 요구한다. 자가 지도 학습이나 자가 훈련 기법과 결합하는 것이 데이터 효율성을 높일 방법으로 제시된다.

5. 평가 방법의 한계: CLIP의 개발 과정에서 전체 검증 데이터를 반복적으로 사용해 참된 zero-shot 시나리오와는 거리가 있으며, 기존 감독 학습 데이터셋에 의존한 평가 또한 한계로 지적된다.

6. 사회적 편향: 인터넷의 이미지-텍스트 쌍에서 학습하면서 CLIP은 편향된 사회적 인식을 포함할 위험이 있으며, 이에 대한 상세한 분석과 해결 방안이 필요하다.

7. Few-Shot 학습과의 부조화: CLIP은 zero-shot 학습에서는 강력하지만, few-shot 학습에서는 성능이 저하될 수 있다. 사람은 zero-shot에서 few-shot으로 넘어갈 때 성능이 크게 향상되므로, CLIP의 zero-shot 성능과 few-shot 효율성을 결합하는 연구가 필요하다.